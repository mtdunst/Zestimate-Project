---
title: "Zestimate Project"
author: "Michael Dunst & Kemi Richards"
date: "`r Sys.Date()`"
output: 
html_document:
  theme: cosmo
    toc: true
    toc_depth: 3
    toc_float: true
  code_folding: hide 
---

```{r setup, include=FALSE, install= TRUE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
# Load some libraries
rm(list = ls())
library(tidycensus)
library(viridis)
library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
library(stargazer)
options(scipen=999) #scientific notation off

# Functions and data directory
census_api_key("8c8e36c4b5046c4d7f8a5d9f0f7a7d0ddde86e8b")

root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")

```
# Introduction
Welcome! **The purpose of this project is to produce an algorithm (model) that can help the online real-estate website, Zillow.com, predict home sale prices with greater accuracy.** Improving real-estate valuations are important for several reasons. Namely it:

1. Improves user experience 
2. Provides context for anticipated property taxes and homeowner's insurance
3. Alleviates historic and systemic biases commonplace with home valuations in neighborhoods of color

However, for this project we were **not** allowed to use sales data towards our algorithm. This rule added a level of difficulty because sales data is a strong predictor towards home valuation (e.g. the sales' price of your neighbor's home is likely very similar to the sales price of your home). We were able to make up for this by emphasizing other variables such as # of floors, bedrooms, bathrooms, the quality of nearby schools, as well as a home's proximity to nearby grocery stores. Overall, our model is sufficient and can be strengthened with a few modifications. For instance, **[XXXXXX]**

# Data
To gather data, our team focused on Mecklenberg County, NC (Charlotte Metro Area) and sourced information from the county's open data website, as well as the American Community Survey and U.S.Census.

## Charlotte Home Sales Data
To begin, we will import a home sales dataset that includes variables like location, housing characteristics, and home quality for the Charlotte Metro Area. After, we will 'clean' our data by creating useful columns such as, "building grade" as a numeric value *(where higher values correspond to greater quality)*, "age of home (age)", "price per square foot (pricesqft)" and calculating the # of "total baths (totbaths)" by joining full and half-bathroom information. Moving forward, we'll refer to this home sales data as **"internal variables."** 
```{r}
#Import sales data, remove columns we won't need, add useful columns, set CRS for North Carolina
CLT_internal <- 
  st_read("https://github.com/mafichman/MUSA_508_Lab/raw/main/Midterm/data/2022/studentData.geojson") %>%
  st_transform('ESRI:103500')

CLT_internal <- CLT_internal[c(5,9,20,21,26,28,30:46,57:60,67,68,70,71,72)] %>%
  mutate(
    age = 2022 - yearbuilt,
    sqft = (totalac * 43560),
     pricesqft = ((totalac * 43560)/price),
        totbaths = (halfbaths*0.5)+(fullbaths))

  CLT_internal$quality <- recode(CLT_internal$bldggrade, MINIMUM = 1 , FAIR = 2, AVERAGE = 3, GOOD = 4, VERYGOOD = 5, EXCELLENT = 6, CUSTOM = 7)
```

## Adding Amenities Data
To build a strong algorithm (model), it's important to include variables that relate to the housing market such as local schools, grocery stores, and parks. We'll refer to these variables as **"amenities."**

```{r tigris_use_cache= TRUE}
# Adding school data 
CLT_schools <- 
  st_read("https://github.com/mtdunst/Zestimate-Project/raw/main/Schools.geojson") %>%
  st_transform(st_crs(CLT_internal))

# Adding grocery store data
CLT_grocery <- 
  st_read("Grocery_pts.geojson") %>%
  st_transform(st_crs(CLT_internal))

# Adding parks data 
CLT_parks <- 
  st_read("https://github.com/mtdunst/Zestimate-Project/raw/main/Parks.geojson") %>%
  st_transform(st_crs(CLT_internal))

```

## Adding Spatial Structure Data 
Finally, we will add variables that provide demographic and environmental data for the Charlotte Metro Area. Specifically, we will include educational attainment, household income, and neighborhoods data. We'll refer to these variables as **"spatial structure."**
```{r}
# Adding demographic data
CLT_demo <- 
  get_acs(geography = "tract", 
          variables = c("B19013_001E", "B15003_022E","B15003_001E"), 
          year=2020, state=37, county=119, 
          geometry=TRUE, output="wide") %>%
  st_transform(st_crs(CLT_internal)) %>%
  dplyr::select( -NAME, -B19013_001M, -B15003_022M, -B15003_001M)

CLT_demo <-
  CLT_demo %>%
  rename(HH_inc = B19013_001E, 
         College = B15003_022E,
         College_age_pop = B15003_001E) %>%
  mutate(college_perc = College/College_age_pop) %>%
  dplyr::select( -College, -College_age_pop)

# Adding neighborhood data 
CLT_neighborhoods <- 
  st_read("https://github.com/mtdunst/Zestimate-Project/raw/main/School_districts.geojson") %>%
  st_transform(st_crs(CLT_internal))
```

## Orienting Our Variables 
So far, we have added *internal*, *amenities*, and *spatial structure* variables. However, in order to build our model and analyze how these variables relate to home sales, we must modify them. We'll achieve this using 2 techniques:

**1. K-nearest neighbor (KNN):** this will find the distance between a given home and the most near amenities (school, grocery store, park).
**2. Spatial join (SJ):** this will join our spatial structure data (educational attainment, neighborhoods) to our internal varies (Charlotte homes sales)

*Note to instructor: the nn_function did not work as normal, perhaps due to the geometry featuring  multiple points (versus just X and Y), so we took the central point of each feature.* 

```{r}
# Most near school, grocery store, and park 
CLT_internal <-
  CLT_internal %>% 
    mutate(
      school_nn1 = nn_function(st_coordinates(st_centroid(CLT_internal)), st_coordinates(st_centroid(CLT_schools)),k = 1),
      grocery_nn1 = nn_function(st_coordinates(st_centroid(CLT_internal)), st_coordinates(st_centroid(CLT_grocery)), k = 1),
       park_nn1 = nn_function(st_coordinates(st_centroid(CLT_internal)), st_coordinates(st_centroid(CLT_parks)), k = 1))

# Spatial join 
CLT_internal <- 
  st_join(CLT_internal,CLT_demo) 

CLT_internal <- 
  st_join(CLT_internal, CLT_neighborhoods)
```

## Summary Statistics **CHECK**
Below are summary statistics tables for each variable category (internal, amenities,spatial structure). 
```{r summary stats}
#Internal variables 
ss_internal <- CLT_internal
ss_internal <- st_drop_geometry(ss_internal)
ss_internal <- ss_internal %>%
  dplyr::select("sqft","pricesqft", "totbaths", "yearbuilt") 
stargazer(as.data.frame(ss_internal), type="html", digits=1, title = "Descriptive Statistics for Charlotte Metro Area Homes Internal Variables")

#Amenities 
ss_amenities <- CLT_internal
ss_amenities <- st_drop_geometry(ss_amenities)
ss_amenities <- ss_amenities %>%
  dplyr::select("school_nn1", "grocery_nn1", "park_nn1") 
stargazer(as.data.frame(ss_internal), type="html", digits=1, title = "Descriptive Statistics for Charlotte Metro Area Homes Amentity Variables")

# Spatial Structure
ss_spatial <- CLT_internal
ss_spatial <- st_drop_geometry(ss_spatial)
ss_spatial <- ss_spatial %>%
  dplyr::select("HH_inc", "college_perc", "FID") 
stargazer(as.data.frame(ss_internal), type="html", digits=1, title = "Descriptive Statistics for Charlotte Metro Area Homes Internal Variables")
```

## Correlation Matrix
Below is a table visualizing correlation between our variables. We can see the home price maintains a positive correlation with the following variables (in order of strength):

* heated square footed of the home
* home quality 
* total bathrooms
* household income
* percentage of residents with college degrees 
* bedrooms
* number of fireplaces in the home

We can use this matrix to inform our variable selection for our model.
```{r corrmatrix}
numericVars <- st_drop_geometry(CLT_internal) %>%
  dplyr::select(price, age, sqft, quality, pricesqft, totbaths, bedrooms, numfirepla, heatedarea, yearbuilt, school_nn1, grocery_nn1, park_nn1, HH_inc, college_perc, FID) %>%
  select(where(is.numeric)) %>%
  na.omit()

ggcorrplot(
  round(cor(numericVars), 1),
  p.mat = cor_pmat(numericVars),
  colors = c("deepskyblue", "grey100", "firebrick1"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation Matrix of Numeric Variables") +
  plotTheme()
```

## Scatterplots 
Below are 4 home price correlation scatterplots based upon the results of our correlation matrix:
```{r scatter}
st_drop_geometry(CLT_internal) %>% 
  dplyr::select(price, quality, heatedarea, HH_inc, yearbuilt) %>% 
    filter(price < 10000000) %>%
  gather(Variable, Value, -price) %>% 
   ggplot(aes(Value, price)) +
     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "hotpink") +
     facet_wrap(~Variable, ncol = 3, scales = "free") +
     labs(title = "Price as a function of Internal and Spatial Variables") 
          + plotTheme()
```
## Maps
Below are 4 maps including:
1. A map of our dependent variable (price) 
2. A map of park locations
3. A map of nearby grocery stores
4. A map of school quality

*Note: the first 3 maps are in relation to home prices within the Charlotte Metro Area.*

### Map 1: Home Price
```{r price}
ggplot() +
  geom_sf(data = CLT_neighborhoods, fill = "grey40") +
  geom_sf(data = CLT_internal, aes(colour = q5(price)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   labels=qBr(CLT_internal,"price"),
                   name="Quintile\nBreaks") +
  labs(title="Home Price", subtitle="Charlotte Metro Area") +
  labs(color = "Observed Sales Price (quintiles)") +
  mapTheme()
```

### Map 2: Parks vs Home Price 
```{r parks}
ggplot() +
  geom_sf(data = CLT_neighborhoods, fill = "grey70") +
  geom_sf(data = CLT_internal, aes(colour = q5(price)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   labels=qBr(CLT_internal,"price"),
                   name="Quintile\nBreaks") +
  geom_sf(data = CLT_parks, color="darkgreen") +
  labs(title="Park Locations vs Home Price", subtitle="Charlotte Metro Area") +
  mapTheme()
```
### Map 3: Grocery Stores vs Home Price
```{r grocery}
ggplot() +
  geom_sf(data = CLT_internal, fill = "grey30") +
  geom_sf(data = CLT_internal, aes(colour = q5(price)), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   labels=qBr(CLT_internal,"price"),
                   name="Quintile\nBreaks") +
  geom_sf(data = CLT_grocery, color="deepskyblue") +
  labs(title="Grocery Store Locations vs Home Price", subtitle="Charlotte Metro Area") +
  mapTheme()
```
### Map 4: School Quality
```{r schoolqual}
ggplot() +
  geom_sf(data = CLT_schools, aes(fill=factor(Quality), color=factor(Quality))) +
  scale_fill_brewer(palette="RdYlGn") +
  scale_color_brewer(palette="RdYlGn") +
  labs(title="School Quality", subtitle="Charlotte Metro Area") +
  mapTheme()
```

# Methods
# Results
# Conclusion
## Gathering

# Time to Train!
Now that we have identified important variables, we will build out model by dividing the data into training/test sets. 

*Before dividing the dataset we will create categorical variables for the number of floors, bathrooms, and bedrooms for a given home.*
```{r floors}
#Re-engineering data as categorical: number of floors
CLT_internal<- 
  CLT_internal%>%
  mutate(NUM_FLOORS.cat = ifelse(storyheigh == "1 STORY" | storyheigh == "1.5 STORY" | storyheigh == "SPLIT LEVEL" | storyheigh == "2.0 STORY", "Up to 2 Floors",
               ifelse(storyheigh == "2.5 STORY" | storyheigh == "3.0 STORY", "Up to 3 Floors", "4+ Floors")))
```

```{r beds}
#Re-engineer bedroom as categorical
CLT_internal <- 
  CLT_internal %>%
  mutate(NUM_BEDS.cat = ifelse(bedrooms <= 2, "Up to 2 Bedrooms",
                               ifelse(bedrooms == 3 | bedrooms == 4, "Up to 4 Bedrooms", "5+ Bedrooms")))
```

```{r baths}
#Re-engineer bathroom data as categorical
CLT_internal <- 
  CLT_internal %>%
  mutate(NUM_BATHS.cat = ifelse(totbaths <= 2.5, "Up to 2.5 Bathrooms",
                               ifelse(totbaths <= 3.5 | totbaths <= 4.5, "Up to 4 Bathrooms", "5+ Bathrooms")))
```

## Dividing the Data
Our data will be partitioned as a 60/40 train-test split. After, we'll run a regression on our training set (60%) and use the results to determine the generalizability with our 'test' data.
```{r}
#Creating training data
inTrain <- createDataPartition(
              y = paste(CLT_internal$NUM_FLOORS.cat, CLT_internal$NUM_BEDS.cat), 
              p = .60, list = FALSE)
charlotte.training <- CLT_internal[inTrain,] 
charlotte.test <- CLT_internal[-inTrain,]  

reg.training <- lm(price ~ ., data = st_drop_geometry(charlotte.training) %>% 
                                    dplyr::select(price, heatedarea, 
                                               quality, NUM_FLOORS.cat,
                                               NUM_BEDS.cat, NUM_BATHS.cat, 
                                               park_nn1, grocery_nn1,
                                               age, HH_inc, college_perc))
summary(reg.training)
```

## Evaluating Generalizability
To test the strength (accuracy) of our model in its ability to predict prices, we will:

1. Find the **mean absolute error (MAE)** + **mean absolute percentage error (MAPE),** 
2. Conduct cross-validation tests
3. Plot predicted prices as a function of observed prices
4. Map our test set residuals, including a **Moran’s I test and a plot** of the spatial lag in errors
5. Map of mean absolute percentage error (MAPE) by neighborhood.
6. Create a scatterplot plot of MAPE by neighborhood as a function of mean price by neighborhood

### MAE & MAPE 
```{r}
#Creating predictions and calculating Mean Absolute Error (MAE) and Mean Absolute Percent Error (MAPE)
charlotte.test <-
  charlotte.test %>%
  mutate(price.Predict = predict(reg.training, charlotte.test),
         price.Error = price.Predict - price,
         price.AbsError = abs(price.Predict - price),
         price.APE = (abs(price.Predict - price)) / price.Predict)%>%
  filter(price < 5000000)

MAE <- mean(charlotte.test$price.AbsError, na.rm = T)
MAPE <- mean(charlotte.test$price.APE, na.rm = T)

reg.MAE.MAPE <- 
  cbind(MAE, MAPE) %>%
  kable(caption = "Regression MAE & MAPE") %>%
  kable_styling("hover",full_width = F) 

reg.MAE.MAPE
```
### Cross Validation
```{r}
#Cross-validation
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(price ~ ., data = st_drop_geometry(CLT_internal) %>% 
                                dplyr::select(price, heatedarea, 
                                               quality, NUM_FLOORS.cat,
                                               NUM_BEDS.cat, NUM_BATHS.cat, grocery_nn1,
                                               age, HH_inc, college_perc, 
                                               park_nn1), 
     method = "lm", trControl = fitControl, na.action = na.pass)

summary(reg.cv)
```

```{r}
#Visualizing prediction errors
charlotte.APE <- charlotte.test[c(6,36,43:46)]

charlotte_APE.sf <- 
  charlotte.APE %>%
  filter(price.APE > 0) %>%
  st_as_sf(sf_column_name=geometry) %>%
  st_transform('ESRI:103500')

ggplot() +
  geom_sf(data = neighborhoods.sf, fill = "grey40") +
  geom_sf(data = charlotte_APE.sf, aes(color = price.APE), size = .25) +
  labs(title="Predicted Sales Price\nAbsolute Percent Error") +
  binned_scale(aesthetics = "color",
               scale_name = "stepsn", 
               palette = function(x) c("#1a9641", "#a6d96a", "#ffffbf", "#fdae61", "#d7191c"),
               breaks = c(0.10, 0.20, 0.5, 0.75),
               limits = c(0, 50),
               show.limits = TRUE, 
               guide = "colorsteps"
  ) +
  scalebar(neighborhoods.sf, dist=5, dist_unit="mi", st.size = 3, location="bottomleft", transform=FALSE) +
  mapTheme()
#Predicted vs observed sales price
ggplot(
  charlotte_APE.sf, aes(price, price.Predict, col = price.APE)) +
  binned_scale(aesthetics = "color",
               scale_name = "stepsn", 
               palette = function(x) c("#1a9641", "#a6d96a", "#ffffbf", "#fdae61", "#d7191c"),
               breaks = c(0.10, 0.20, 0.5, 0.75),
               limits = c(0, 50),
               show.limits = TRUE, 
               guide = "colorsteps"
  ) +
    geom_point(size=1) +
  scale_y_continuous(limits = c(0, 4000000)) +
  scale_x_continuous(limits = c(0, 4000000)) +
  labs(title="Sales Price vs. Predicted", subtitle="Charlotte Metro Area") +
  ylab("Predicted Sales Price (in dollars)") +
  xlab("Observed Sales Price (in dollars)") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black", size = 0.5) +
  labs(color = "Absolute % Error") +
  geom_label(
    label="0% error line", 
    x=3500000,
    y=3000000,
    label.padding = unit(0.25, "lines"), # Rectangle size around label
    label.size = 0.15,
    color = "black",
    fill="grey80")
```

```{r}
#Calculating Moran's I
coords.test <-  st_coordinates(charlotte.test) 

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")
 
charlotte.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, price.Error, NAOK = TRUE)) %>%
  ggplot(aes(lagPriceError, price.Error))

moranTest <- moran.mc(charlotte.test$price.Error, 
                      spatialWeights.test, nsim = 999, na.action=na.exclude)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()
```

```{r}
#Adjusting for neighborhod
reg.nhood <- lm(price ~ ., data = as.data.frame(charlotte.training) %>% 
                                 dplyr::select(price, heatedarea, 
                                               quality, NUM_FLOORS.cat,
                                               NUM_BEDS.cat, NUM_BATHS.cat, 
                                               park_nn1, grocery_nn1, MIDD_NAME,
                                               age, HH_inc, college_perc))
summary(reg.nhood)

charlotte.test.nhood <-
  charlotte.test %>%
  mutate(Regression = "Neighborhood Effects",
         price.Predict = predict(reg.nhood, charlotte.test),
         price.Error = price.Predict- price,
         price.AbsError = abs(price.Predict- price),
         price.APE = (abs(price.Predict- price)) / price)%>%
  filter(price < 5000000)

charlotte.test <-charlotte.test %>%
  mutate(Regression = "Baseline")

sales_predictions.sf <- CLT_internal %>%
  mutate(price.Predict = predict(reg.nhood, CLT_internal)) %>%
  filter(toPredict == "CHALLENGE")

sales_predictions.df <- as.data.frame(st_drop_geometry(sales_predictions.sf))
sales_predictions.df <- sales_predictions.df[c(30,43)]


write.csv(sales_predictions.df,"C:/Users/14145/OneDrive/Documents/GitHub/Zestimate-Project/Quisqueyanes.csv", row.names = FALSE)
  
```

```{r}
bothRegressions <- 
  rbind(
    dplyr::select(charlotte.test, starts_with("price"), Regression, MIDD_NAME) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, price.Error, NAOK=TRUE)),
    dplyr::select(charlotte.test.nhood, starts_with("price"), Regression, MIDD_NAME) %>%
      mutate(lagPriceError = lag.listw(spatialWeights.test, price.Error, NAOK=TRUE)))    
```

```{r}
#Neighborhood effect results
bothRegressions %>%
  dplyr::select(price.Predict, price, Regression) %>%
    ggplot(aes(price, price.Predict)) +
  geom_point() +
  stat_smooth(aes(price, price), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(aes(price.Predict, price), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  facet_wrap(~Regression) +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  plotTheme()
```


```{r}
#Scatter plot of MAPE by neighborhood mean price
npa.mean.sf <- left_join(
  st_drop_geometry(charlotte.test) %>%
    group_by(npa) %>%
    summarize(meanPrice = mean(price, na.rm = T)),
  mutate(charlotte.test, predict.fe = 
                        predict(lm(price ~ npa, data = charlotte.test), 
                        charlotte.test)) %>%
    st_drop_geometry %>%
    group_by(npa) %>%
      summarize(meanAPE = mean(price.APE)))

ggplot(npa.mean.sf, aes(x=meanPrice, y=meanAPE)) +
  geom_point(alpha=0.5) +
  scale_y_continuous(limits = c(0, 2)) +
  scale_x_continuous(limits = c(0, 2500000)) +
  labs(title="Sales Price vs. Prediction Error", subtitle="Charlotte Area Home Sales") +
  ylab("Mean Absolute % Error") +
  xlab("Observed Sales Price")
```


### MULTIVARIATE CORRELATION
```{r}
#Re-running multivariate correlation
reg1 <- lm(price ~ ., data = st_drop_geometry(CLT_internal) %>% 
                                 dplyr::select(price, park_nn1, grocery_nn1, NUM_BATHS.cat, heatedarea, quality, age, NUM_BEDS.cat, numfirepla, HH_inc, college_perc, NUM_FLOORS.cat))

summary(reg1)
```


